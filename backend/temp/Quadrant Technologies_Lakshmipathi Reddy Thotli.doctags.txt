<doctag><picture><loc_355><loc_23><loc_447><loc_38></picture>
<section_header_level_1><loc_60><loc_50><loc_184><loc_58>Lakshmipathi Reddy Thotli</section_header_level_1>
<text><loc_59><loc_68><loc_443><loc_149>Objective: I believe there is nothing great apart from the right living. I always focus on listening the client requirements and working smart to provide a very sophisticated solution besides coordinating with team mates. I am good at writing Pyspark code in Databricks Notebooks, Spark SQL queries, creating workflows, and orchestrating notebooks using Azure Data Factory developing Power BI reports with little experience in SSRS reports too. I am a very simple, transparent and happy person to listen, understand, and discuss with teammates. I am looking for a good company where I can bring value to achieve their goals besides experiencing perpetual learning.</text>
<section_header_level_1><loc_59><loc_158><loc_116><loc_165>Competencies</section_header_level_1>
<unordered_list><list_item><loc_74><loc_174><loc_310><loc_181>Languages: T-SQL (70-761 certified), Python, JavaScript )</list_item>
<list_item><loc_74><loc_186><loc_334><loc_192>ETL: SQL Server Integration Services (SSIS), Azure Data Factory</list_item>
<list_item><loc_74><loc_197><loc_173><loc_203>Scripting: Shell script.</list_item>
<list_item><loc_74><loc_208><loc_214><loc_213>Data Platform: Azure Databricks</list_item>
<list_item><loc_74><loc_219><loc_206><loc_226>Distributed Computing: Spark</list_item>
<list_item><loc_74><loc_230><loc_190><loc_237>Reporting: SSRS, Power BI</list_item>
<list_item><loc_74><loc_241><loc_288><loc_248>Cloud: Azure (ADLS, Blob storage, Databricks, ADF)</list_item>
<list_item><loc_74><loc_252><loc_249><loc_258>Version Control Tools: TFS and Bit bucket</list_item>
<list_item><loc_74><loc_263><loc_378><loc_270>IDE Tools: SQL Server Management Studio (SSMS) and Visual Studio Code</list_item>
</unordered_list>
<section_header_level_1><loc_59><loc_290><loc_223><loc_297>Certifications - 70 - 761: T - SQL Querying</section_header_level_1>
<text><loc_59><loc_306><loc_311><loc_312>Tech Mahindra, GSK - Senior Data Engineer - 2022-08 - Current</text>
<text><loc_60><loc_321><loc_156><loc_328>Project: GlaxoSmithKline</text>
<text><loc_59><loc_337><loc_394><loc_344>Technologies: Azure Databricks (Pyspark, spark SQL), Azure Data Factory, Shell Script.</text>
<text><loc_59><loc_353><loc_443><loc_370>Description: GSK is a global biopharma company. Our mission is to migrate the on-premise system to Azure cloud environment and optimize ETL process.</text>
<section_header_level_1><loc_60><loc_379><loc_150><loc_386>Key Accomplishments:</section_header_level_1>
<unordered_list><list_item><loc_74><loc_396><loc_368><loc_402>Implemented prototype of the project in order to showcase the clients</list_item>
<list_item><loc_74><loc_407><loc_354><loc_413>Developed ETL framework using Databricks and Azure Data Factory</list_item>
<list_item><loc_74><loc_418><loc_236><loc_425>Implemented medallion architecture.</list_item>
<list_item><loc_74><loc_429><loc_253><loc_436>Loading source files to Blob (Event based)</list_item>
<list_item><loc_74><loc_440><loc_302><loc_447>Processing files (Encrypt, Decrypt, Unzip and Validate)</list_item>
<list_item><page_break></list_item>
<list_item><loc_74><loc_51><loc_308><loc_57>Read processed files from ADLS in Databricks Notebook</list_item>
<list_item><loc_74><loc_62><loc_245><loc_68>Schema enforcement and data cleaning</list_item>
<list_item><loc_74><loc_73><loc_216><loc_79>Load data (Full and Incremental)</list_item>
<list_item><loc_74><loc_84><loc_282><loc_90>Implement transformations to create data marts.</list_item>
<list_item><loc_74><loc_95><loc_443><loc_112>Lead team during implementation phase and contributed immensely regarding technical aspects of the project.</list_item>
</unordered_list>
<section_header_level_1><loc_59><loc_132><loc_249><loc_139>Accenture -Data Engineer - 2021- 04 - 2022-01.</section_header_level_1>
<text><loc_60><loc_148><loc_163><loc_154>Hyperion Insurance Group</text>
<text><loc_60><loc_164><loc_233><loc_170><loc_59><loc_179><loc_146><loc_186>Project: Hyperion, a multinational insurance company based in U.K</text>
<text><loc_59><loc_195><loc_430><loc_202>Technologies: Server (T-SQL), SSIS, Azure (Data Factory, Blob Storage, Databricks), JSON, YAML</text>
<text><loc_59><loc_211><loc_443><loc_260>Description: Hyperion is an international insurance intermediary group with insurance broking and underwriting companies under its umbrella. Our mission is to provide robust and complete solutions to store, process, and modelling data to cater needs of providing insights and also building excellent visual reports (Power BI, Angular reports). Hybrid data model is used in this project to provide live analytics .</text>
<section_header_level_1><loc_60><loc_270><loc_150><loc_276>Key Accomplishments:</section_header_level_1>
<unordered_list><list_item><loc_74><loc_286><loc_316><loc_292>CRUD operations using T-SQL as per project requirements</list_item>
<list_item><loc_74><loc_297><loc_256><loc_303>Developed Pipelines in Azure Data Factory</list_item>
<list_item><loc_74><loc_308><loc_356><loc_315>Created SSIS packages to import data from source to MS SQL server</list_item>
<list_item><loc_74><loc_319><loc_385><loc_326>Implementation of Lift and Shift technique to migrate SSIS packages to ADF</list_item>
<list_item><loc_74><loc_330><loc_328><loc_337>Created complex queries to provide data models for reports.</list_item>
<list_item><loc_74><loc_341><loc_366><loc_348>Employed data cleansing methods, significantly Enhanced data quality</list_item>
</unordered_list>
<text><loc_59><loc_368><loc_291><loc_374>GGK Technologies - Software Engineer 2020-01 - 2021-03</text>
<text><loc_59><loc_384><loc_80><loc_389>AARP</text>
<text><loc_60><loc_399><loc_303><loc_406>Project: AARP, a non-profit social interest Organization in USA</text>
<text><loc_59><loc_415><loc_193><loc_422>Technologies: Shell scripting, AWS</text>
<picture><loc_355><loc_23><loc_447><loc_38></picture>
<page_break>
<picture><loc_355><loc_23><loc_447><loc_38></picture>
<text><loc_59><loc_50><loc_443><loc_78>Description: This system handles File Transfer operations from vendors to business partners. Files are processed at AWS servers to provide the files to business clients in required format (zipping, unzipping, renaming, encryption, decryption etc.).</text>
<section_header_level_1><loc_60><loc_87><loc_150><loc_94>Key Accomplishments:</section_header_level_1>
<unordered_list><list_item><loc_74><loc_103><loc_298><loc_110>Developed shell scripts to perform all the operations.</list_item>
<list_item><loc_74><loc_115><loc_363><loc_121>Automated the process to send email notifications about file transfer.</list_item>
<list_item><loc_74><loc_126><loc_387><loc_132>Worked on processing and transforming files to suit business requirements.</list_item>
<list_item><loc_74><loc_137><loc_443><loc_154>Created a flow to run the jobs without human intervention based on time and day of the month using Cron utility.</list_item>
</unordered_list>
<section_header_level_1><loc_59><loc_163><loc_291><loc_170>GGK Technologies, Software Engineer - 2018-08 - 2019-12</section_header_level_1>
<text><loc_59><loc_179><loc_116><loc_184>Globus Brands</text>
<text><loc_60><loc_195><loc_285><loc_201>Project: GFOB, escorted tour company based in Australia.</text>
<text><loc_59><loc_211><loc_265><loc_217>Technologies: T-SQL, MS SQL server, SSIS,Azure Data</text>
<text><loc_60><loc_226><loc_224><loc_233>Factory, Azure Databricks, SSRS, Power BI.</text>
<text><loc_59><loc_242><loc_443><loc_291>Description: GFOB is a group of companies providing escorted tour, river cruise and independent travel package services across multiple countries. Our mission is to bring the data from oracle server to MS SQL Server. Migrate existing citrus reports to SSRS reports with logic and UI enhancements. Create Power BI reports on top of data model according to the business requirements to find Key Performance Indicators.</text>
<section_header_level_1><loc_60><loc_301><loc_150><loc_307>Key Accomplishments:</section_header_level_1>
<unordered_list><list_item><loc_74><loc_317><loc_216><loc_323>Developed ETL workflow in SSIS.</list_item>
<list_item><loc_74><loc_328><loc_383><loc_335>Created SSIS packages to import data from Oracle server to MS SQL server.</list_item>
<list_item><loc_74><loc_339><loc_301><loc_346>Created job schedules using SQL server Agent service.</list_item>
<list_item><loc_74><loc_350><loc_284><loc_357>Created stored procedures, views to get datasets.</list_item>
<list_item><loc_74><loc_361><loc_404><loc_368>Created stored procedures, views to get datasets for SSRS and Power BI reports.</list_item>
<list_item><loc_74><loc_372><loc_443><loc_390>Upgraded the same functionality to cloud using Azure services (ADLS,Data Factory, Databricks).</list_item>
</unordered_list>
<section_header_level_1><loc_60><loc_399><loc_98><loc_404>Education</section_header_level_1>
<unordered_list><list_item><loc_74><loc_415><loc_441><loc_422>B. Tech: CSE G. Pulla Reddy Engineering College - Kurnool CGPA: 7.82 - 2014-07 - 2018-07</list_item>
<list_item><loc_74><loc_426><loc_430><loc_433>Intermediate: MPC Sri Gayatri Junior College - Kurnool Marks: 956 - 2011-04 - 2013-05</list_item>
<list_item><loc_74><loc_437><loc_414><loc_444>Class X: CBSE Jawahar Navodaya Vidyalaya - Kadapa CGPA: 9.2 - 2010-04 - 2011-06</list_item>
</unordered_list>
</doctag>
<doctag><section_header_level_1><loc_224><loc_44><loc_267><loc_50>Manoj Bhat</section_header_level_1>
<section_header_level_1><loc_61><loc_70><loc_123><loc_76>Career Objective</section_header_level_1>
<text><loc_61><loc_84><loc_410><loc_115>Seeking an opportunity to continuously enhance my skills in data engineering, data warehousing, and data governance, while actively participating in the design and implementation of innovative data- driven solutions that drive business growth and competitiveness.</text>
<section_header_level_1><loc_61><loc_123><loc_153><loc_128>Executive Summary</section_header_level_1>
<unordered_list><list_item><loc_76><loc_137><loc_419><loc_174>As a Data Engineer with over 5+ years of experience, I specialize in designing and implementing data solutions. Proficient in Python, Microsoft Fabric, PySpark, SQL, Azure, Pandas, and Apache Spark, and I have a track record of creating scalable data pipelines, optimizing workflows, and maintaining data integrity.</list_item>
<list_item><loc_76><loc_176><loc_411><loc_190>My problem-solving abilities are strong, and I possess a comprehensive understanding of big data technologies and cloud platforms.</list_item>
<list_item><loc_76><loc_192><loc_409><loc_213>My responsibilities have included developing and managing ETL pipelines using ADF and PySpark, managing large-scale data processing and transformation tasks effectively.</list_item>
<list_item><loc_76><loc_215><loc_430><loc_229>I have experience in designing and implementing data warehouse solutions on Azure Synapse.</list_item>
<list_item><loc_76><loc_231><loc_425><loc_252>Additionally, I have developed medallion architecture, data integration workflowsto facilitate seamless data movement across varioussystems and platforms.</list_item>
<list_item><loc_76><loc_255><loc_391><loc_276>I have conducted comprehensive data quality assessments and implemented data cleansing and validation procedures to ensure data accuracy and integrity.</list_item>
<list_item><loc_76><loc_278><loc_390><loc_299>Working closely with cross-functional teams, I have provided technicalsolutionsfor data -driven initiatives by understanding and addressing data requirements effectively.</list_item>
</unordered_list>
<section_header_level_1><loc_61><loc_315><loc_119><loc_321>Analytical Skills</section_header_level_1>
<unordered_list><list_item><loc_76><loc_329><loc_436><loc_350>Git/ GitHub, Jira/ Version one, Linux, Windows, Python Pandas, NumPy, Matplotlib, Machine Learning, Palantir, Gen-AI LLM, Microsoft Fabric, PySpark, MySQL and Power-BI, DAX,Power query</list_item>
<list_item><loc_76><loc_360><loc_395><loc_374>Azure Data Bricks, Azure Data Factory (ADF), Azure SQL Database, Blob storage, Azure data lake storage, Azure Synapse</list_item>
</unordered_list>
<section_header_level_1><loc_61><loc_390><loc_132><loc_396>Management Skills</section_header_level_1>
<text><loc_61><loc_404><loc_381><loc_417>Stakeholder & Client Management, Project & Team Management, Agile Practitioner, Communication Skills.</text>
<section_header_level_1><loc_61><loc_424><loc_143><loc_429>Technical Certification</section_header_level_1>
<text><loc_61><loc_441><loc_385><loc_446>Microsoft Certified – DP203, PL300, AZ-900, DP 900, AI 900, DP 600, DP 700</text>
<picture><loc_379><loc_7><loc_482><loc_24></picture>
<page_break>
<section_header_level_1><loc_61><loc_58><loc_145><loc_64>Professional Summary</section_header_level_1>
<text><loc_61><loc_72><loc_99><loc_78>Project - 1</text>
<text><loc_61><loc_81><loc_212><loc_87>Company - Softline Solution Pvt. Ltd.</text>
<text><loc_61><loc_91><loc_102><loc_95>Duration</text>
<text><loc_102><loc_93><loc_105><loc_94>-</text>
<text><loc_108><loc_91><loc_207><loc_96>May 2022 to Feb 2025</text>
<text><loc_61><loc_100><loc_88><loc_104>Client</text>
<text><loc_89><loc_102><loc_91><loc_103>-</text>
<text><loc_95><loc_100><loc_206><loc_105>United Health Group USA</text>
<text><loc_61><loc_109><loc_99><loc_114>Position –</text>
<text><loc_99><loc_112><loc_104><loc_112>–</text>
<text><loc_107><loc_109><loc_167><loc_115>Data Engineer</text>
<text><loc_61><loc_119><loc_362><loc_133>Technical Skills: Python, SQL, Azure, PySpark, ADF, ADLS Gen2, Fabric Description</text>
<text><loc_61><loc_137><loc_421><loc_206>The project involved developing an ETL pipeline to integrate patient records, treatment history, billing information, and hospital resource utilization data from various source systems (Electronic Health Records (EHR), Lab systems, Billing systems) into a centralized data warehouse. The primary goal was to enable comprehensive reporting and analytics for patient care, operational efficiency, and cost management. The ETL system transformed raw healthcare data into structured formats for easy analysis, improving the decision-making process for both clinical and administrative functions. It ensured HIPAA compliance and followed stringent data governance standards to secure sensitive patient information.</text>
<text><loc_61><loc_217><loc_413><loc_246>The pipeline processed data such as patient admissions, discharges, lab results, medication records, insurance claims, and staff allocation to improve patient care quality and optimize resource use. This system provided dashboards and detailed reports for hospital administrators, doctors, and staff.</text>
<section_header_level_1><loc_61><loc_257><loc_128><loc_262>Responsibilities</section_header_level_1>
<unordered_list><list_item><loc_76><loc_264><loc_415><loc_286>Developed ETL processes to extract, transform, and load healthcare data from multiple source systems (EHR, lab systems, billing systems) into a centralized data warehouse using ADF and Databricks.</list_item>
<list_item><loc_76><loc_287><loc_412><loc_307>Applied data cleansing techniquesto ensure accuracy and consistency, and implemented complex transformation logic using PySpark and SQL to normalize and aggregate patient records, treatment, and billing data.</list_item>
<list_item><loc_76><loc_308><loc_421><loc_321>Tuned ETL workflowsfor optimal performance, reducing data processing time and optimizing resource utilization in Azure Databricks clusters.</list_item>
<list_item><loc_76><loc_322><loc_407><loc_335>Implemented error-handling mechanisms, monitoring, and logging for ETL jobs, ensuring high availability and reliability of the data pipelines.</list_item>
<list_item><loc_76><loc_336><loc_401><loc_349>Integrated Azure Key Vault to manage sensitive credentials and maintain HIPAA compliance.</list_item>
<list_item><loc_76><loc_351><loc_418><loc_365>Developed ETL processes to extract, transform, and load healthcare data from multiple</list_item>
<list_item><loc_76><loc_368><loc_400><loc_382>source systems (EHR, lab systems, billing systems) into a centralized data warehouse using</list_item>
<list_item><loc_76><loc_385><loc_270><loc_390>ADF, Databricks, and Fabric environment.</list_item>
<list_item><loc_76><loc_394><loc_426><loc_408>Applied data cleansing techniques to ensure accuracy and consistency and implemented</list_item>
<list_item><loc_76><loc_411><loc_394><loc_425>complex transformation logic using PySpark and SQL to normalize and aggregate patient records, treatment, and billing data.</list_item>
<list_item><loc_76><loc_434><loc_430><loc_448>Tuned ETL workflows for optimal performance, reducing data processing time and optimizing resource utilization in Azure Databricks clusters.</list_item>
<list_item><loc_76><loc_53><loc_435><loc_67>Implemented error-handling mechanisms, monitoring, and logging for ETL jobs, ensuring</list_item>
<list_item><loc_91><loc_70><loc_306><loc_75>high availability and reliability of the data pipelines.</list_item>
<list_item><loc_76><loc_78><loc_434><loc_92>Integrated Azure Key Vault to manage sensitive credentials and maintain HIPAA compliance.</list_item>
<list_item><loc_76><loc_95><loc_435><loc_117>Design, develop, and maintain robust data pipelines using SQL to extract, transform, and load (ETL) data from various sources into data warehouses or data lakes.</list_item>
<list_item><loc_76><loc_121><loc_422><loc_126>Optimize and troubleshoot existing SQL queries for performance and reliability.</list_item>
<list_item><loc_76><loc_129><loc_435><loc_143>Write efficient and complex DAX expressions to support advanced business calculations and KPIs in Power BI reports.</list_item>
</unordered_list>
<page_break>
<picture><loc_379><loc_7><loc_482><loc_24></picture>
<picture><loc_379><loc_7><loc_482><loc_24></picture>
<section_header_level_1><loc_61><loc_155><loc_100><loc_161>Project – 2</section_header_level_1>
<text><loc_61><loc_172><loc_170><loc_176>Client: HCA Healthcare UK</text>
<text><loc_61><loc_180><loc_156><loc_186>Position: Data Engineer</text>
<text><loc_61><loc_189><loc_330><loc_195>Technical Skills: ADF, Python, SQL, Azure, PySpark, ADLS Gen2</text>
<text><loc_61><loc_198><loc_440><loc_250>Description: This project involved migrating legacy hospital data systems to a modern cloud -based architecture to improve the scalability and performance of data processing. The project aimed to consolidate patient information, medical records, and operational data from various on -premise systems & SFTP file system into a cloud data warehouse on Azure. By migrating to a modern ETL framework, the hospital could streamline data management, enhance reporting capabilities.</text>
<section_header_level_1><loc_61><loc_259><loc_130><loc_265>Responsibilities:</section_header_level_1>
<unordered_list><list_item><loc_85><loc_268><loc_428><loc_289>Developed ETL pipelines using Azure Data Factory to migrate data from legacy on- premise systems to Azure Data Lake and Azure SQL Database, ensuring minimal downtime during migration.</list_item>
<list_item><loc_85><loc_291><loc_417><loc_311>Standardized data from different hospital systems (EHR, billing, and lab data) by implementing transformation rules using Data bricks and PySpark, ensuring uniformity in the migrated data for improved analytics.</list_item>
<list_item><loc_85><loc_312><loc_405><loc_331>Conducted thorough data validation checks post-migration to ensure accuracy and consistency of the migrated data, identifying and resolving discrepancies between the legacy and new systems.</list_item>
<list_item><loc_85><loc_333><loc_405><loc_353>Optimized large data transfers by utilizing parallel processing in ADF and partitioning techniques in Data bricks, reducing the migration time significantly while maintaining data integrity.</list_item>
</unordered_list>
<section_header_level_1><loc_61><loc_368><loc_97><loc_373>Education</section_header_level_1>
<section_header_level_1><loc_61><loc_383><loc_80><loc_387>MCA</section_header_level_1>
<text><loc_61><loc_392><loc_315><loc_414>Swami Vivekananda Institute of Technology & Management University Punjab, India 2016 – 2018</text>
</doctag>